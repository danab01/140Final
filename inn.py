# -*- coding: utf-8 -*-
"""INN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ret6Y60Lz0EwR_AZASxdE1MdmVIKJHwE
"""

from google.colab import drive
drive.mount('/content/drive')

import os 

os.chdir('/content/drive/MyDrive/Academics/~Harvard/3.JR./NEURO_140')

from IPython.display import HTML
import matplotlib.pyplot as plt
from matplotlib import cm
import numpy as np
import itertools
from utils2 import *
from flow2 import *
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
import pandas

x_dim = 36
y_dim = 1
z_dim = 35
tot_dim = y_dim + z_dim
pad_dim = tot_dim - x_dim

n_data = 5000
n_couple_layer = 2
n_hid_layer = 2
n_hid_dim = 36

n_batch = 500
n_epoch = 2000
n_display = 100

# load dataset
dataframe = pandas.read_csv("DM_data2.csv", delim_whitespace=True, header=None)
dataset = dataframe.values
# split into input (X) and output (Y) variables
X = dataset[:,:-1]
Y = dataset[:,-1]

Y2 = []
for y in Y:
  w = []
  w.append(y)
  Y2.append(w)

x_data = X.astype('float32')
z = np.random.multivariate_normal([0.]*z_dim, np.eye(z_dim), X.shape[0])
y_data = np.concatenate([z, Y2], axis=-1).astype('float32')

z.shape

# Make dataset generator
x_data = tf.data.Dataset.from_tensor_slices(x_data)
y_data = tf.data.Dataset.from_tensor_slices(y_data)
dataset = (tf.data.Dataset.zip((x_data, y_data))
           .shuffle(buffer_size=X.shape[0])
           .batch(n_batch, drop_remainder=True)
           .repeat())

model = NVP(tot_dim, n_couple_layer, n_hid_layer, n_hid_dim, name='NVP')
x = tfk.Input((tot_dim,))
model(x);
model.summary()

class Trainer(tfk.Model):
    def __init__(self, model, x_dim, y_dim, z_dim, tot_dim, 
                 n_couple_layer, n_hid_layer, n_hid_dim, shuffle_type='reverse'):
        super(Trainer, self).__init__()
        self.model = model
        self.x_dim = x_dim
        self.y_dim = y_dim
        self.z_dim = z_dim
        self.tot_dim = tot_dim
        self.x_pad_dim = tot_dim - x_dim
        self.y_pad_dim = tot_dim - (y_dim + z_dim)
        self.n_couple_layer = n_couple_layer
        self.n_hid_layer = n_hid_layer
        self.n_hid_dim = n_hid_dim
        self.shuffle_type = shuffle_type

        self.w1 = 5.
        self.w2 = 1.
        self.w3 = 10.
        self.loss_factor = 1.
        self.loss_fit = MSE
        self.loss_latent = MMD_multiscale
        self.loss_backward = MMD_multiscale

    def train_step(self, data):
        x_data, y_data = data
        x = x_data[:, :self.x_dim]
        y = y_data[:, -self.y_dim:]
        z = y_data[:, :self.z_dim]
        y_short = tf.concat([z, y], axis=-1)

        # Forward loss
        with tf.GradientTape() as tape:
            y_out = self.model(x_data)    
            pred_loss = self.w1 * self.loss_fit(y_data[:,self.z_dim:], y_out[:,self.z_dim:]) # [zeros, y] <=> [zeros, yhat]
            output_block_grad = tf.concat([y_out[:,:self.z_dim], y_out[:, -self.y_dim:]], axis=-1) # take out [z, y] only (not zeros)
            latent_loss = self.w2 * self.loss_latent(y_short, output_block_grad) # [z, y] <=> [zhat, yhat]
            forward_loss = pred_loss + latent_loss
        grads_forward = tape.gradient(forward_loss, self.model.trainable_weights)
        self.optimizer.apply_gradients(zip(grads_forward, self.model.trainable_weights))

        # Backward loss
        with tf.GradientTape() as tape:
            x_rev = self.model.inverse(y_data)
            #rev_loss = self.w3 * self.loss_factor * self.loss_fit(x_rev, x_data)
            rev_loss = self.w3 * self.loss_factor * self.loss_backward(x_rev, x_data)
        grads_backward = tape.gradient(rev_loss, self.model.trainable_weights)
        self.optimizer.apply_gradients(zip(grads_backward, self.model.trainable_weights)) 

        total_loss = forward_loss + latent_loss + rev_loss
        return {'total_loss': total_loss,
                'forward_loss': forward_loss,
                'latent_loss': latent_loss,
                'rev_loss': rev_loss}

    def test_step(self, data):
        x_data, y_data = data
        return NotImplementedError

trainer = Trainer(model, x_dim, y_dim, z_dim, tot_dim, n_couple_layer, n_hid_layer, n_hid_dim)
trainer.compile(optimizer='Adam')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# LossFactor = UpdateLossFactor(n_epoch)
# logger = NBatchLogger(n_display, n_epoch)
# hist = trainer.fit(dataset,
#                    batch_size=n_batch,
#                    epochs=n_epoch,
#                    steps_per_epoch=n_data//n_batch, 
#                    callbacks=[logger, LossFactor], 
#                    verbose=0)

fig, (ax1,ax2) = plt.subplots(1, 2, facecolor='white', figsize=(16,5))
ax1.set_ylim(-2,20)
ax1.plot(hist.history['total_loss'], 'k.-', label='total_loss')
ax1.plot(hist.history['forward_loss'], 'b.-', label='forward_loss')
ax2.plot(hist.history['latent_loss'], 'g.-', label='latent_loss')
ax2.plot(hist.history['rev_loss'], 'r.-', label='inverse_loss')
plt.legend()

# Stores predictions from every score included in training data
z = np.random.multivariate_normal([1.]*z_dim, np.eye(z_dim), Y.shape[0])
y = np.concatenate([z, Y2], axis=-1).astype('float32')
x_pred = model.inverse(y).numpy()

y2 = [100]
z2 = np.random.multivariate_normal([1.]*z_dim, np.eye(z_dim))
y3 = np.concatenate([z2, y2], axis=-1).astype('float32')

t = model.inverse(y3)

t

